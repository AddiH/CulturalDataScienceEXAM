{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7942caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # Til at hente hjemmesider\n",
    "from bs4 import BeautifulSoup # Til at trække information ud af den dowloadede html\n",
    "from urllib.parse import urljoin\n",
    "import json # Til at parse json\n",
    "from time import sleep\n",
    "#from tqdm.notebook import tqdm # Processbar\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path # Til at arbejde med filer\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b9276",
   "metadata": {},
   "source": [
    "Det første vi gør er at finde en liste over alle kredsene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61184cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmeseddel_url = 'https://www.dr.dk/nyheder/politik/folketingsvalg/din-stemmeseddel'\n",
    "stemmeseddel_req = requests.get(stemmeseddel_url) # Hent siden\n",
    "stemmeseddel_soup = BeautifulSoup(stemmeseddel_req.text) # Ved at sætte .text får man html koden fra siden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5052a35",
   "metadata": {},
   "source": [
    "Hiv links til kredsene ud af siden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bba060e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kreds_urls_ugly = stemmeseddel_soup('a', class_='AccordionGrid_link__cGkec') # extracting the links 'a' we want. By looking at the page with the inspect feature, we see that all the links we're interested in are classified with AccordionGrid_link__cGkec.\n",
    "kreds_urls = list(map(lambda x: urljoin(stemmeseddel_url, x['href']), kreds_urls_ugly)) # ? Combining the stemmeseddel and kreds url... somehow.\n",
    "\n",
    "#kreds_urls = list(map(lambda x: urljoin(stemmeseddel_url, x['href']), stemmeseddel_soup('a', class_='AccordionGrid_link__cGkec')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105783ef",
   "metadata": {},
   "source": [
    "På hver kreds-side er der en liste over alle kandidaterne, så alle de lister finder vi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66eba454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kreds_kandidater(kreds_url):\n",
    "    req = requests.get(kreds_url) # Hent kreds-siden\n",
    "    soup = BeautifulSoup(req.text) # Strukturer html, så vi kan søge i den\n",
    "    \n",
    "    kandidater_soup = soup.find('script', id='__NEXT_DATA__').text # don't get this part\n",
    "    kandidater_json = json.loads(kandidater_soup) # Træk json ud af siden\n",
    "    kandidater_data = kandidater_json['props']['pageProps']['smallConstituencyCandidatesByPartyCode'] # udtræk kun kandidat info fra json\n",
    "    \n",
    "    # constituents_data[0] er alle kandiater fra første parti i kredsen (i JSON format). I kreds_urls[0] - Rønne, er det socialkrabaterne, Lea, Steen og Lars.\n",
    "    \n",
    "    # Find alle url'er\n",
    "    kandidat_urls = []\n",
    "    for parti in kandidater_data: # gå i gennem alle partier i kredsen\n",
    "        for kandidat in list(parti.values())[0]['candidates']: # for alle kandidater i partiet\n",
    "            kandidat_urls.append(kandidat['urlKey']) # gem kandidatens URL\n",
    "    return kandidat_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a305d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 92/92 [03:04<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "kandidat_urls = []\n",
    "for kreds_url in tqdm(kreds_urls): # gå igennem alle kredse\n",
    "    kandidat_urls += kreds_kandidater(kreds_url) # i kreds, gem kandidat URL\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c9285e",
   "metadata": {},
   "source": [
    "Vi gemmer alle links til kandidaterne i en fil, så vi kan bruge det senere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1aeae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open(\"candidate_urls.txt\", \"x\")\n",
    "urlfile = Path('kandidat_urls') # path() \n",
    "with urlfile.open('w') as f: # with automatically closes the file again, avoiding us closing it again manually\n",
    "                             # the 'w' allows ud to write in the file\n",
    "                             # with open as f, the file is in the variable f\n",
    "    for url in set(kandidat_urls): # candidate_urls indeholder alle kandidaterne flere gange fordi vi ikke har begrænset os til storkredsene, så vi fjerner dubletter ved at bruge 'set'\n",
    "        f.write(url + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408fddd4",
   "metadata": {},
   "source": [
    "Nu kan vi hente data om hver enkelt kandidat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6eb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "kandidat_baseurl = 'https://www.dr.dk/nyheder/politik/folketingsvalg/din-stemmeseddel/kandidater/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa2590a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1014/1014 [16:24<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for kandidat_url in tqdm(set(kandidat_urls)):\n",
    "    kandidat_fil = Path(f'kandidater/{kandidat_url}.json') # Vi vil gemme data i en fil\n",
    "    if kandidat_fil.exists(): continue #skip resten af loop\n",
    "\n",
    "    req = requests.get(kandidat_baseurl + kandidat_url) # Hent siden\n",
    "    soup = BeautifulSoup(req.text)\n",
    "    kandidat_data = soup.find('script', id='__NEXT_DATA__').text # Træk data ud\n",
    "    kandidat_fil.write_text(kandidat_data) # Gem det write_text laver filen\n",
    "    sleep(random.random() * 8) # Vi sover lidt for ikke at spamme serveren (det er nok ikke så vigtigt givet størrelsen på DR, men høfligst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499ec9a-e0be-4316-8623-ab8826545e86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
